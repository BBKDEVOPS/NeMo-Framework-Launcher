run:
  name: nemo_imagen_sr1024_400m
  results_dir: ${base_results_dir}/${.name}
  time_limit: "14-00:00:00"
  dependency: "singleton"

name: nemo_imagen # The name of your model
allow_tf32: True
 
trainer:
  devices: 8 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]
  num_nodes: 8
  max_epochs: -1
  max_steps: 1250000 # precedence over max_epochs
  logger: False  # Provided by exp_manager 
  replace_sampler_ddp: False
  precision: bf16 # Should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  log_every_n_steps: 5  # Interval of logging.
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  num_sanity_val_steps: 10 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # Provided by exp_manager
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True


exp_manager:
  explicit_log_dir: ${training.run.results_dir}/results
  exp_dir: null
  name: ${training.name}
  create_wandb_logger: False
  wandb_logger_kwargs: # Whether you want exp_manger to create a Wandb logger
    name: ${training.run.name}
    project: nemo_imagen
  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger
  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback
  checkpoint_callback_params:
    monitor: reduced_train_loss
    save_top_k: 5
    every_n_epochs: 0 # Save checkpoint frequency.
    every_n_train_steps: 1000 # Mutually exclusive with every_n_epochs. It is recommended to set this if training on large-scale dataset.
    filename: '${training.name}--{reduced_train_loss:.2f}-{step}-{consumed_samples}'
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  ema:
    enable: True
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False
model:

  precision: ${training.trainer.precision}
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 64 # limited by GPU memory
  global_batch_size: 4096 # will use more micro batches to reach global batch size
  inductor: False
  inductor_cudagraphs: False

  unet_type: sr
  unet:
    embed_dim: 128
    image_size: 1024
    channels: 3
    channel_mult: [1, 1, 2, 4, 8]
    num_attn_heads: 8
    per_head_channels: 64
    attention_type: cross
    atnn_enabled_at: [0, 0, 0, 0, 1]
    feature_pooling_type: attention
    stride: 2
    num_resblocks: [1, 2, 4, 8, 8]
    learned_sinu_pos_emb_dim: 0
    use_null_token: False
    init_conv_kernel_size: 3
    gradient_checkpointing: False
    scale_shift_norm: True
    stable_attention: True
    flash_attention: False

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)

  noise_scheduler:
    noise_schedule: cosine
    timesteps: 1000

  pred_objective: noise
  loss_type: l2

  conditioning:
    embed_dim: 1024
    token_length: 128
    drop_rate: 0.1
    precached_key: embeddings_t5_xxl
    out_key: t5_text
    
  data:
    num_workers: 16
    train:
      dataset_path:
        - datasets/coyo-700m-1024/wdinfo-selene.pkl
        - datasets/laion_aesthetic-1024/wdinfo-selene.pkl
      augmentations:
        resize_smallest_side: 1024
        center_crop_h_w: 256, 256
        horizontal_flip: False
      filterings: null
      corruption_aug:
        target_resolution: [64, 256]
        kernel_radius_dict:     # used for blurring & resizing, otherwise, not necessary.
              8: 1
              16: 2
              32: 3
              64: 6
              128: 11
              256: 22
              512: 44
              1024: 88
              2048: 176
              4096: 352
        blur:
          add_random_blur: True
          blur_prob1: 0.25
          blur_prob2: 0.25

          blur_sigma_dict:
              8:  0.0375
              16: 0.075
              32: 0.1125
              64: 0.225
              128: 0.45
              256: 0.9
              512: 1.8
              1024: 3.6
              2048: 7.2
              4096: 14.4

        resize:
          add_random_resize: True
          resize_prob1:
              up: 0.2
              down: 0.35
              keep: 0.45
          resize_prob2:
              up: 0.2
              down: 0.2
              keep: 0.6

          resize_range1:
              - 0.8
              - 1.2
          resize_range2:
              - 0.8
              - 1.2

        noise:
          add_random_noise: True
          gaussian_noise_prob1: 0.8     # 0.5
          gaussian_noise_prob2: 0.8     # 0.5
          gray_noise_prob1: 0.0         # 0.4
          gray_noise_prob2: 0.0         # 0.4

          gaussian_sigma_range1:
              - 0
              - 3
          gaussian_sigma_range2:
              - 0
              - 2.5

          poisson_scale_range1:
              - 0.005
              - 0.3
          poisson_scale_range2:
              - 0.005
              - 0.25

        jpeg:
          add_random_compression: True
          jpeg_range1:
              - 75
              - 95
          jpeg_range2:
              - 75
              - 95 


    webdataset:
      infinite_sampler: True
      local_root_path: /datasets

  optim:
    # We need weight decay for large-scale odel
    name: fused_adam
    lr: 0.0001
    eps: 1e-8
    betas: [0.9, 0.999]
    weight_decay: 0.01
    sched:
        name: WarmupPolicy
        warmup_steps: 10000
        warmup_ratio: null