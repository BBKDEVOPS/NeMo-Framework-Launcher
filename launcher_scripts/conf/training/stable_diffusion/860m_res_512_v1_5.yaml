run:
  name: stable_diffusion_860m_res_512_v1_5
  results_dir: ${base_results_dir}/${.name}
  time_limit: "3-12:00:00"
  dependency: "singleton"

name: nemo_stable_diffusion

trainer:
  devices: 8
  num_nodes: 32
  accelerator: gpu
  precision: 16
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  replace_sampler_ddp: True
  max_epochs: -1 # PTL default. In practice, max_steps will be reached first.
  max_steps: 165000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 10
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True
  strategy:
    bucket_cap_mb: 256
    gradient_as_bucket_view: True
    find_unused_parameters: False
    allreduce_precision: 32

exp_manager:
  exp_dir: null
  name: ${name}
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: stable-diffusion
    group: nemo-sd
    name: ${name}
    resume: True
  create_checkpoint_callback: True
  create_tensorboard_logger: True
  checkpoint_callback_params:
    every_n_train_steps: 1000
    every_n_epochs: 0
    monitor: train/loss
    filename: '${name}--{train/loss:.4f}-{epoch}-{step}'
    save_nemo_on_train_end: False
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  ema:
    enable: True
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False


model:
  base_learning_rate: 1.0e-4
  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 200
  timesteps: 1000
  first_stage_key: images
  cond_stage_key: captions # txt for cifar, caption for pbss
  image_size: 64
  channels: 4
  cond_stage_trainable: false
  conditioning_key: crossattn # check
  monitor: val/loss_simple_ema
  scale_factor: 0.18215
  use_ema: False
  scale_by_std: False
  ckpt_path:
  ignore_keys: []
  parameterization: eps
  clip_denoised: True
  load_only_unet: False
  cosine_s: 8e-3
  given_betas:
  original_elbo_weight: 0
  v_posterior: 0
  l_simple_weight: 1
  use_positional_encodings: False
  learn_logvar: False
  logvar_init: 0
  beta_schedule: linear
  loss_type: l2
  learning_rate: 1.0e-04
  concat_mode: True
  cond_stage_forward:
  text_embedding_dropout_rate: 0.1
  fused_opt: True
  inductor: True
  inductor_cudagraphs: False

  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    from_pretrained: /path/to/v1-2.ckpt
    from_NeMo: True #Must be specified when from pretrained is not None, False means loading unet from HF ckpt
    image_size: 32 # unused
    in_channels: 4
    out_channels: 4
    model_channels: 320
    attention_resolutions:
    - 4
    - 2
    - 1
    num_res_blocks: 2
    channel_mult:
    - 1
    - 2
    - 4
    - 4
    num_heads: 8
    use_spatial_transformer: true
    transformer_depth: 1
    context_dim: 768
    use_checkpoint: False
    legacy: False
    use_flash_attention: True

  first_stage_config:
    _target_: nemo.collections.multimodal.models.stable_diffusion.ldm.autoencoder.AutoencoderKL
    from_pretrained: /path/to/vae.bin
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      double_z: true
      z_channels: 4
      resolution: 256  #Never used
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity

  cond_stage_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenCLIPEmbedder
    version: # verison of clip text encoder
    device: cuda
    max_length: 77


  scheduler_config:
    cls: nemo.collections.multimodal.parts.stable_diffusion.lr_scheduler.LambdaLinearScheduler
    warm_up_steps: [ 0 ]
    cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
    f_start: [ 1.0e-6 ]
    f_max: [ 1.e-4 ]
    f_min: [ 1.e-10 ]


  data:
      num_workers: 16
      train:
          batch_size: 32
          dataset_path:
            - ${data_dir}/your_dataset/wdinfo.pkl
          augmentations:
            resize_smallest_side: 512
            center_crop_h_w: 512, 512
            horizontal_flip: False
          filterings:

      webdataset:
          infinite_sampler: False
          local_root_path: ${data_dir}/your_dataset/tarfiles_reorganized/  # each tarfile in wdinfo is relative to this
